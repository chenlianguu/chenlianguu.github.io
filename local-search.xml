<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Apache Airflow使用指南</title>
    <link href="/2020/01/09/Apache-Airflow%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"/>
    <url>/2020/01/09/Apache-Airflow%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>airflow 是 apache下孵化项目，是纯 Python 编写的一款非常优雅的开源调度平台。github 上有 8971 个星，是非常受欢迎的调度工具。airflow 使用 DAG (有向无环图) 来定义工作流，配置作业依赖关系非常方便，豪不夸张地说：方便程度简直甩其他任务调度工具一条街。<br> airflow 有着以下天然优势：</p><ol><li>灵活易用，airflow 本身是 Python 编写的，且工作流的定义也是 Python 编写，有了 Python 胶水的特性，没有什么任务是调度不了的，有了开源的代码，没有什么问题是无法解决的，你完全可以修改源码来满足个性化的需求，而且更重要的是代码都是 <strong>–human-readable</strong> 。</li><li>功能强大，自带的 Operators 都有15+，也就是说本身已经支持 15+ 不同类型的作业，而且还是可自定义 Operators，什么 shell 脚本，python，mysql，oracle，hive等等，无论不传统数据库平台还是大数据平台，统统不在话下，对官方提供的不满足，完全可以自己编写 Operators。</li><li>优雅，作业的定义很简单明了, 基于 jinja 模板引擎很容易做到脚本命令参数化，web 界面更是也非常 <strong>–human-readable</strong> 。</li><li>极易扩展，提供各种基类供扩展, 还有多种执行器可供选择，其中 CeleryExcutor 使用了消息队列来编排多个工作节点(worker), 可分布式部署多个 worker ，airflow 可以做到无限扩展。</li><li>丰富的命令工具，你甚至都不用打开浏览器，直接在终端敲命令就能完成测试，部署，运行，清理，重跑，追数等任务。</li></ol><p>airflow 是免费的，可以将一些常做的巡检任务，定时脚本（如 crontab ），ETL处理，监控等任务放在 airflow 上集中管理，甚至都不用再写监控脚本，作业出错会自动发送日志到指定人员邮箱，低成本高效率地解决生产问题。</p><h2 id="组成部分"><a href="#组成部分" class="headerlink" title="组成部分"></a>组成部分</h2><p>从一个使用者的角度来看，调度工作都有以下功能：</p><ol><li>系统配置（$AIRFLOW_HOME/airflow.cfg）</li><li>作业管理（$AIRFLOW_HOME/dags/xxxx.py）</li><li>运行监控（webserver)</li><li>报警（邮件或短信）</li><li>日志查看（webserver 或 $AIRFLOW_HOME/logs/***)</li><li>跑批耗时分析（webserver)</li><li>后台调度服务（scheduler)</li></ol><p>除了短信需要自己实现，其他功能 airflow 都有，而且在 airflow 的 webserver 上我们可以直接配置数据库连接来写 sql 查询，做更加灵活的统计分析。</p><h2 id="重要概念"><a href="#重要概念" class="headerlink" title="重要概念"></a>重要概念</h2><h3 id="DAG"><a href="#DAG" class="headerlink" title="DAG"></a>DAG</h3><p>Linux 的 crontab 和 windows 的任务计划，他们可以配置定时任务或间隔任务，但不能配置作业之前的依赖关系。airflow 中 DAG 就是管理作业依赖关系的。DAG 的英文 directed acyclic graphs 即有向无环图，下图 1 便是一个简单的 DAG<br><img src="https://upload-images.jianshu.io/upload_images/12989993-96bcaf5716827e99?imageMogr2/auto-orient/strip%7CimageView2/2/w/657/format/webp" srcset="/img/loading.gif" alt="img"></p><p>在 airflow 中这种 DAG 是通过编写 Python 代码来实现的，DAG 的编写非常简单，官方提供了很多的例子，在安装完成后，启动 webserver 即可看到 DAG 样例的源码（其实定义了 DAG 对象的 python 程序），稍做修改即可成为自己的 DAG 。上图 1 中 DAG 中的依赖关系通过下述三行代码即可完成：<br><img src="https://upload-images.jianshu.io/upload_images/12989993-755318fef8cd6dd4?imageMogr2/auto-orient/strip%7CimageView2/2/w/766/format/webp" srcset="/img/loading.gif" alt="img"></p><h3 id="Operators-操作符"><a href="#Operators-操作符" class="headerlink" title="Operators-操作符"></a>Operators-操作符</h3><p>DAG 定义一个作业流，Operators 则定义了实际需要执行的作业。airflow 提供了许多 Operators 来指定我们需要执行的作业：</p><ol><li>BashOperator - 执行 bash 命令或脚本。</li><li>SSHOperator - 执行远程 bash 命令或脚本（原理同 paramiko 模块）。</li><li>PythonOperator - 执行 Python 函数。</li><li>EmailOperator - 发送 Email。</li><li>HTTPOperator - 发送一个 HTTP 请求。</li><li>MySqlOperator, SqliteOperator, PostgresOperator, MsSqlOperator, OracleOperator, JdbcOperator, 等. - 执行 SQL 任务。</li><li>DockerOperator, HiveOperator, S3FileTransferOperator, PrestoToMysqlOperator, SlackOperator 等。</li></ol><p>除了以上这些 Operators 还可以方便的自定义 Operators 满足个性化的任务需求。<br>后续会介绍如何使用这些 Operators。</p><h3 id="Timezone-时区"><a href="#Timezone-时区" class="headerlink" title="Timezone-时区"></a>Timezone-时区</h3><p>airflow 1.9 之前的版本使用本地时区来定义任务开始日期，scheduler_interval 中 crontab 表达式中的定时也是依据本地时区为准，但 airflow 1.9 及后续新版本将默认使用 UTC 时区来确保 airflow 调度的独立性，以避免不同机器使用不同时区导致运行错乱。如果调度的任务集中在一个时区上，或不同机器，但使用同一时区时，需要对任务的开始时间及 cron 表达式进行时区转换，或直接使用本地时区。目前 1.9 的稳定版本还不支持时区配置，后续版本会加入时区配置，以满足使用本地时区的需求。</p><h3 id="Webserver-Web服务器"><a href="#Webserver-Web服务器" class="headerlink" title="Webserver-Web服务器"></a>Webserver-Web服务器</h3><p>webserver 是 airflow 的界面展示，可显示 DAG 视图，控制作业的启停，清除作业状态重跑，数据统计，查看日志，管理用户及数据连接等。不运行 webserver 并不影响 airflow 作业的调度。</p><h3 id="Schduler-调度器"><a href="#Schduler-调度器" class="headerlink" title="Schduler-调度器"></a>Schduler-调度器</h3><p>调度器 schduler 负责读取 DAG 文件，计算其调度时间，当满足触发条件时则开启一个执行器的实例来运行相应的作业，必须持续运行，不运行则作业不会跑批。</p><h3 id="Worker-工作节点"><a href="#Worker-工作节点" class="headerlink" title="Worker-工作节点"></a>Worker-工作节点</h3><p>当执行器为 CeleryExecutor 时，需要开启一个 worker。</p><h3 id="Executor-执行器"><a href="#Executor-执行器" class="headerlink" title="Executor-执行器"></a>Executor-执行器</h3><p>执行器有 SequentialExecutor, LocalExecutor, CeleryExecutor</p><ol><li>SequentialExecutor 为顺序执行器，默认使用 sqlite 作为知识库，由于 sqlite 数据库的原因，任务之间不支持并发执行，常用于测试环境，无需要额外配置。</li><li>LocalExecutor 为本执行器，不能使用 sqlite 作为知识库，可以使用 mysql,postgress,db2,oracle 等各种主流数据库，任务之间支持并发执行，常用于生产环境，需要配置数据库连接 url。</li><li>CeleryExecutor 为 Celery 执行器，需要安装 Celery ,Celery 是基于消息队列的分布式异步任务调度工具。需要额外启动工作节点-worker。使用 CeleryExecutor 可将作业运行在远程节点上。</li></ol><hr><h2 id="基于Docker搭建"><a href="#基于Docker搭建" class="headerlink" title="基于Docker搭建"></a>基于Docker搭建</h2><p>基于第三方docker镜像进行安装，github-repo：<a href="https://github.com/puckel/docker-airflow" target="_blank" rel="noopener">https://github.com/puckel/docker-airflow</a></p><p>前提：机器已经装上docker及docker-compose命令，本页面基于Docker搭建，部署airflow的CeleryExecutor模式</p><ul><li><p>Step1 拉取代码</p><pre><code class="shell">git clone https://github.com/puckel/docker-airflow.git</code></pre></li><li><p>Step2 修改 docker-compose-CeleryExecutor.yml及Dockerfile</p><pre><code>cd ~/docker-airflowvim docker-compose-CeleryExecutor.yml</code></pre><p>默认的compose是抓原来版本的镜像包，改成latest标签，这样才会使用用Dockerfile构建的镜像</p><p>修改web暴露端口，8080可能被占用，下图修改为8096端口</p></li></ul><ul><li><p>Step3 Airflow Config设置</p><pre><code>cd configvim airflow.cfg</code></pre></li></ul><ul><li><p>构建镜像</p><pre><code class="shell">docker build --rm -t puckel/docker-airflow:latest .docker-compose -f docker-compose-CeleryExecutor.yml up -d</code></pre></li></ul><p>当看到下图，就代表已经启动，可通过该端口进行访问</p>]]></content>
    
    
    <categories>
      
      <category>etl</category>
      
    </categories>
    
    
    <tags>
      
      <tag>docker</tag>
      
      <tag>etl</tag>
      
      <tag>apache airflow</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Apache Nifi使用指南</title>
    <link href="/2020/01/09/Apache-Nifi%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"/>
    <url>/2020/01/09/Apache-Nifi%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</url>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Apache NiFi是什么？NiFi官网给出如下解释：“一个易用、强大、可靠的数据处理与分发系统”。通俗的来说，即Apache NiFi 是一个易于使用、功能强大而且可靠的数据处理和分发系统，其为数据流设计，它支持高度可配置的指示图的数据路由、转换和系统中介逻辑。</p><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>单节点架构</p><p><img src="https://nifi.apache.org/docs/nifi-docs/html/images/zero-master-node.png" srcset="/img/loading.gif" alt="NiFi Architecture Diagram"></p><p>集群架构图</p><p><img src="https://nifi.apache.org/docs/nifi-docs/html/images/zero-master-cluster.png" srcset="/img/loading.gif" alt="NiFi Cluster Architecture Diagram"></p><p><strong>web-sever</strong></p><p>其目的在于提供基于HTTP的命令和控制API。</p><p><strong>Flow Controller</strong></p><p>这是操作的核心，以Processor为处理单元，提供了用于运行的扩展线程，并管理扩展接收资源时的调度。</p><p><strong>Extensions</strong></p><p>在其他文档中描述了各种类型的NiFi扩展，Extensions的关键在于扩展在JVM中操作和执行。</p><p><strong>FlowFile Repository</strong></p><p>FlowFile库的作用是NiFi跟踪记录当前在流中处于活动状态的给定流文件的状态，其实现是可插拔的，默认的方法是位于指定磁盘分区上的一个持久的写前日志。FlowFile库的作用是NiFi跟踪记录当前在流中处于活动状态的给定流文件的状态，其实现是可插拔的，默认的方法是位于指定磁盘分区上的一个持久的写前日志。</p><p><strong>Content Repository</strong></p><p>Content库的作用是给定流文件的实际内容字节所在的位置，其实现也是可插拔的。默认的方法是一种相对简单的机制，即在文件系统中存储数据块。</p><p><strong>Provenance Repository</strong></p><p>Provenance库是所有源数据存储的地方，支持可插拔。默认实现是使用一个或多个物理磁盘卷，在每个位置事件数据都是索引和可搜索的。</p><h2 id="docker部署"><a href="#docker部署" class="headerlink" title="docker部署"></a>docker部署</h2><p>nifi需要通过集群实现多租户，在standalone模式下，可以通过docker启动多个实例来实现多用户通过不同端口访问各自的nifi</p><pre><code class="shell">docker run --name nifi01 \  --restart=always \  -p 8090:8080 \  -p 10000:10000 \  -v /data/nifi:/data/nifi/ \  -d \  apache/nifi:latest  docker run --name nifi02 \  --restart=always \  -p 8094:8080 \  -p 10001:10000 \  -v /data/nifi02:/data/nifi02/ \  -d \  apache/nifi:latest</code></pre><p>启动nifi，停止nifi</p><pre><code class="shell">docker start nifidocker stop nifi</code></pre><h2 id="NiFi-Processor"><a href="#NiFi-Processor" class="headerlink" title="NiFi Processor"></a>NiFi Processor</h2><p>Flow Controller是NiFi的核心，Flow Controller扮演者文件交流的处理器角色，维持着多个处理器的连接并管理各个Processer，Processor则是实际处理单元。</p><p><img src="/.com//processor.png" srcset="/img/loading.gif" alt="1572501106736"></p><p>Processor包含各种类型的组件，如amazon、attributes、hadoop等，可通过前缀进行轻易辨识，如Get、Fetch开头代表获取，如getFile、getFTP、FetchHDFS，execute代表执行，如ExecuteSQL、ExecuteProcess、ExecuteFlumeSink等均可较容易知其简单用途。右边选择hadoop则会显示所有hadoop相关的processor，如图所示</p><p><img src="/.com//hadoop.png" srcset="/img/loading.gif" alt="1572501279158"></p><p>与hadoop相关的processor有读写HDFS文件，写Hbase，读写parquet文件等</p><h2 id="Nifi实战Demo"><a href="#Nifi实战Demo" class="headerlink" title="Nifi实战Demo"></a>Nifi实战Demo</h2><h3 id="通过Nifi实现把指定文件夹中的文件移动到另一个文件夹"><a href="#通过Nifi实现把指定文件夹中的文件移动到另一个文件夹" class="headerlink" title="通过Nifi实现把指定文件夹中的文件移动到另一个文件夹"></a>通过Nifi实现把指定文件夹中的文件移动到另一个文件夹</h3><ul><li><p>step 1选取processor</p><p>选取getfile及putfile的process，并连接</p><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/geifile.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/geifile.png" srcset="/img/loading.gif" alt="geifile.png"></a></p></li></ul><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/flow.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/flow.png" srcset="/img/loading.gif" alt="flow.png"></a></p><ul><li><p>step 2 填写getfile及putfile相关属性</p><p>双击getfile或者putfile的processor，property中填写源数据文件夹及目标文件夹，其他参数按需进行配置，此demo按照默认参数填写，对于putfile processor中的setting栏，设置勾选自动终止策略，当putfile失败或者成功的时候就停止此processor</p><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/properties.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/properties.png" srcset="/img/loading.gif" alt="properties.png"></a></p></li></ul><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/putfile.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/putfile.png" srcset="/img/loading.gif" alt="putfile.png"></a></p><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/putfile_config.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/putfile_config.png" srcset="/img/loading.gif" alt="putfile_config.png"></a></p><ul><li><p>step 3 启动</p><p>空白处右键选择start，状态栏显示绿色代表processor在执行，假如/data/nifi/input有文件此时开启工作流后/data/nifi/input对应也会有该文件</p></li></ul><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/start.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/start.png" srcset="/img/loading.gif" alt="start.png"></a></p><h3 id="数据库表和表同步"><a href="#数据库表和表同步" class="headerlink" title="数据库表和表同步"></a>数据库表和表同步</h3><p>表到表的同步,NIFI默认sql查询出来的数据为Avro格式,所以需要先将Avro格式转化为json格式,再将json转换为sql语句,最后使用PUTSQL处理器将数据存入数据库。需要用到ExecuteorSQL、ConvertAvroToJSON、ConvertJSONToSQL、PUTSQL四种处理器。<br><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/table2table.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/table2table.png" srcset="/img/loading.gif" alt="table2table.png"></a> </p><ul><li>step 1 使用ExecuteSQL配置数据源<br>从Components ToolBar上将processor拖拽到画布上，选择ExecuteSQL处理器，右键或双击该处理器编辑properties。该处理器原生提供三种数据库连接池，提供了大多数数据库驱动，另外还可以自定义连接池的名称。<br><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/ExecuteSQL.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/ExecuteSQL.png" srcset="/img/loading.gif" alt="ExecuteSQL.png"></a><ul><li>Database Connection Pooling Service 提供数据库连接池服务  </li><li>postgresql驱动程序<a href="https://jdbc.postgresql.org/download.html" target="_blank" rel="noopener">下载</a><br><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/driverConfig.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/driverConfig.png" srcset="/img/loading.gif" alt="driverConfig.png"></a></li><li>驱动配置完成后点击⚡,使能后查看是否报错  </li></ul></li><li>step 2 使用ConvertAvroToJSON将Avro格式转换为json格式<br><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/ConvertAvroToJSON.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/ConvertAvroToJSON.png" srcset="/img/loading.gif" alt="ConvertAvroToJSON.png"></a></li><li>step 3 使用ConvertJSONToSQL将json数据转换为SQL语句<br><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/ConvertJSONToSQL.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/ConvertJSONToSQL.png" srcset="/img/loading.gif" alt="ConvertJSONToSQL.png"></a> </li><li>step 4 使用PUTSQL将数据存入到数据库，配置完成后数据流启动后可以看到数据流动过程，再到数据库中验证结果。<br><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/PUTSQL.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/PUTSQL.png" srcset="/img/loading.gif" alt="PUTSQL.png"></a></li></ul><p><strong>Tips:基本案例跑通后可以在Operate栏createTemplate,可以在web页面直接导出xml格式的数据处理流程重复使用减少配置时间。</strong>  </p><h2 id="Trouble-Shooting"><a href="#Trouble-Shooting" class="headerlink" title="Trouble Shooting"></a>Trouble Shooting</h2><ul><li>原始文件夹或者目标文件夹没有权限，process右上角会显示报错信息，此时应该将文件夹设置对应权限，将拥有者改为nifi用户，此命令应该用root权限执行，若nifi在docker中，应该到使用root用户登录到对应container执行</li></ul><pre><code>linux版本sudo chown -R nifi:nifi /data/nifi/inputsudo chown -R nifi:nifi /data/nifi/outputdocker版本docker exec -it -u root nifi /bin/bashchown -R nifi:nifi /data/nifi/inputchown -R nifi:nifi /data/nifi/output</code></pre><p>报错信息：</p><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/error.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/error.png" srcset="/img/loading.gif" alt="error.png"></a></p><ul><li>putfile或者getfile的properties填写错误的时候，对应processor上面会显示感叹号，按照提示修改即可，如getfile中源文件夹为/data/nifi/input01，系统中没有此文件夹，则会显示对应提示信息</li></ul><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/error01.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/error01.png" srcset="/img/loading.gif" alt="error01.png"></a></p>]]></content>
    
    
    <categories>
      
      <category>etl</category>
      
    </categories>
    
    
    <tags>
      
      <tag>docker</tag>
      
      <tag>apache nifi</tag>
      
      <tag>etl</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spark run on K8s</title>
    <link href="/2020/01/09/Spark-run-on-K8s/"/>
    <url>/2020/01/09/Spark-run-on-K8s/</url>
    
    <content type="html"><![CDATA[<h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><p>首先查看了下spark的官方文档，了解了spark怎么在k8s上面跑的，实际上不需要搭建spark集群，提交作业到k8s的api server即可。看似简单但是还是不知道怎么动手实践。于是youtube上面搜了下相关视频，按照视频很快就实践了一把spark run on k8s，具体步骤如下：</p><ul><li>step 1 到k8-master机器下载二进制spark最新二进制安装包，并解压</li></ul><pre><code class="shell">cd /optwget http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgztar -zxvf http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz</code></pre><ul><li>step 2 制作spark的docker镜像</li></ul><pre><code class="shell">cd spark-2.4.4-bin-hadoop2.7./bin/docker-image-tool.sh -r chenlianguu -t v2.4.4 build  # 制作spark进行./bin/docker-image-tool.sh -r chenlianguu -t v2.4.4 push  #将spark镜像推送到docker hubdocker images[root@k8s-master opt]# docker imagesREPOSITORY                                                        TAG                 IMAGE ID            CREATED             SIZEchenliang/spark-r                                                 v2.4.4              6479a523e3f7        20 hours ago        759MB</code></pre><ul><li>step 3 提交作业到k8s</li></ul><p>在提交spark的作业的机器上，把api server的proxy打开</p><pre><code class="shell">kubectl proxy</code></pre><pre><code class="shell">./bin/spark-submit \--master k8s://http://127.0.0.1:8001 \--name spark-pi \--deploy-mode cluster \--class org.apache.spark.examples.SparkPi \--conf spark.executor.instances=3 \--conf spark.kubernetes.container.image=chenlianguu/spark-r:v2.4.4 \/opt/spark-2.4.4-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.4.jar</code></pre><h2 id="运行说明"><a href="#运行说明" class="headerlink" title="运行说明"></a>运行说明</h2><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/k8s-cluster-mode.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/k8s-cluster-mode.png" srcset="/img/loading.gif" alt="k8s-cluster-mode.png"></a><br>集群模式下，通过spark-submit提交程序到k8s集群，具体一下步骤：  </p><ul><li>通过k8s创建spark driver端的pod</li><li>driver端在k8s其他节点创建executor端的pod并保持通信，executor具体执行代码，这里设计到权限问题，假如没有对应的权限创建pods，执行spark会报错，具体见trouble  shooting第二点</li><li>当程序跑完了，executor端pod会终止并清理，driver端的pod会保持complete状态并持久化log信息，最后会由k8s api server进行driver端pod的垃圾回收工作</li></ul><h2 id="trouble-shooting"><a href="#trouble-shooting" class="headerlink" title="trouble shooting"></a>trouble shooting</h2><p>跑spark on k8s的pi example碰到的一些坑及解决方案</p><ul><li>找不到jar问题<br><img src="https://i.loli.net/2020/01/09/xzqUmBIuCdP6HlW.jpg" srcset="/img/loading.gif" alt="1"><br><img src="https://i.loli.net/2020/01/09/U6uIFOrgcjQAkn4.jpg" srcset="/img/loading.gif" alt="2"><br>之前提交的脚本是这样的  </li></ul><pre><code class="shell">./bin/spark-submit \--master k8s://http://127.0.0.1:8001 \--name spark-pi \--deploy-mode cluster \--class org.apache.spark.examples.SparkPi \--conf spark.executor.instances=3 \--conf spark.kubernetes.container.image=chenlianguu/spark-r:v2.4.4 \/opt/spark-2.4.4-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.4.jar</code></pre><p>其中jar包制定的地址是local，路径填写应该加上协议local:///opt/spark-2.4.4-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.4.jar，加上之后还是报同样的错，google后发现jar包实际上是在jar包在docker镜像里面的地址，因此改为local:///opt/spark/examples/jars/spark-examples_2.11-2.4.4.jar   </p><ul><li>jar问题解决了，又开始报错，报错信息如下<br><img src="https://i.loli.net/2020/01/09/BgsviowCp5EzARG.jpg" srcset="/img/loading.gif" alt="3"><br>第一感觉就是权限问题，网上找到了解决方案，ref：<a href="https://github.com/GoogleCloudPlatform/continuous-deployment-on-kubernetes/issues/113" target="_blank" rel="noopener">解决办法</a>，按照这个思路，在k8s-master节点，输入   <pre><code>kubectl create clusterrolebinding default --clusterrole cluster-admin --serviceaccount=default:default</code></pre></li></ul><p>然后继续执行spark-submit脚本，可以顺利启动driver端，但是有2个executor端还是报错，说是资源不够，有一个executor端执行成功，整个job还是顺利的跑下来了，查看driver端日志。kubectl logs podsname </p><h2 id><a href="#" class="headerlink" title></a><img src="https://i.loli.net/2020/01/09/5bWGKh34DqVQfYU.jpg" srcset="/img/loading.gif" alt="5"></h2><p>参考资料：<br><a href="https://spark.apache.org/docs/latest/running-on-kubernetes.html" target="_blank" rel="noopener">spark官方doc</a><br><a href="https://www.youtube.com/watch?v=l7UoE97Z24I&list=LL-3gJZTnF4DbSyb7KID7P7g&index=2&t=0s" target="_blank" rel="noopener">youtube动手视频</a></p>]]></content>
    
    
    <categories>
      
      <category>kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>kubernetes</tag>
      
      <tag>大数据</tag>
      
      <tag>部署</tag>
      
      <tag>spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes部署</title>
    <link href="/2020/01/09/Kubernetes%E9%83%A8%E7%BD%B2/"/>
    <url>/2020/01/09/Kubernetes%E9%83%A8%E7%BD%B2/</url>
    
    <content type="html"><![CDATA[<p>k8s的搭建主要有三种方式：kubeadmin安装、docker安装及二进制安装，其中二进制安装方式最为复杂需要部署人员了解网络https，ca证书等方面的知识，kubeadmin安装方式大大简化了部署操作，刚入门建议尝试kubeadmin方式安装。docker安装我并不觉得合适，本身k8s作为容器编排系统部署在docker里面，网络及相关端口配置较为复杂，为了避免埋下太多的坑，本人没有尝试这种方式进行安装。</p><p>具体安装部署我就不写在这里了，分享一套自己看的k8s入门视频，很简短能够快速了解k8s，当时k8s学习曲线是比较陡峭的。</p><p>👉🏻<a href="https://pan.baidu.com/s/1ieZmpZdsae73PacD9FZDYA" target="_blank" rel="noopener">百度云盘</a> 密码:amlo</p><p>👉🏻<a href="https://alevelhome-my.sharepoint.com/:w:/p/chenliang/EbQBPVmKj7VBpKZGYo02h4YBGbutlFV-28jq86GsR76HzQ?e=SgE4o5" target="_blank" rel="noopener">安装部署文档</a>  </p><p>按照此文档部署基本没啥坑，注意的是部分镜像在国外服务器，需要替换成文档作者国内的镜像即可。</p><hr><p>最近看到新的工具rancher，企业级管理运维kubernetes集群的好工具的。后续继续更新</p>]]></content>
    
    
    <categories>
      
      <category>kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>kubernetes</tag>
      
      <tag>部署</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker部署伪分布式大数据环境</title>
    <link href="/2020/01/09/Docker%E9%83%A8%E7%BD%B2%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%8E%AF%E5%A2%83/"/>
    <url>/2020/01/09/Docker%E9%83%A8%E7%BD%B2%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%8E%AF%E5%A2%83/</url>
    
    <content type="html"><![CDATA[<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>通过docker搭建大数据环境  </p><ul><li>开箱即用，不常驻后台，需要的时候启动集群即可，不用的时候关闭集群释放机器资源</li><li>避免了直接安装端口占用问题，大数据平台所需端口较多</li><li>spark资源docker化，便于后期k8s进行资源管理调度</li></ul><p>上述好处主要是基于测试环境下，基于生产环境的大数据平台化要考虑的点很多，例如后期扩容、运维、安全等因素，大数据平台整个是否docker化后期有待考证。</p><h2 id="镜像制作方案"><a href="#镜像制作方案" class="headerlink" title="镜像制作方案"></a>镜像制作方案</h2><p>使用Docker来搭建hadoop,spark及mysql的集群，首先使用Dockerfile制作镜像，把相关的软件拷贝到约定好的目录 下，把配置文件在外面先配置好，再拷贝移动到hadoop,spark的配置目录，为了能使得mysql能从其它节点被访问到，要配置mysql的访问权限。</p><h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p>一共3个节点，即启动3个容器。hadoop-master,hadoop-node1,hadoop-node2这三个容器里面安装hadoop和spark集群。<br><img src="https://i.loli.net/2020/01/09/HIAsFMrPekj4fZw.png" srcset="/img/loading.gif" alt="architect.png"></p><h2 id="集群部署"><a href="#集群部署" class="headerlink" title="集群部署"></a>集群部署</h2><h3 id="集群网络规划及子网配置"><a href="#集群网络规划及子网配置" class="headerlink" title="集群网络规划及子网配置"></a>集群网络规划及子网配置</h3><p>既然是做集群，网络的规划是少不了的,至于网络，可以通过Docker中的DockerNetworking的支持配置。首先设置网络，docker中设置 子网可以通过docker network create 方法，这里我们通过命令设置如下的子网。–subnet指定子网络的网段，并为这个子网命名一个名字叫spark</p><pre><code class="shell"># 创建子网docker network create --subnet=172.16.0.0/16 spark# 查看网络docker network lsNETWORK ID          NAME                       DRIVER              SCOPEfab2dd51d1cf        spark                      bridge              local</code></pre><p> 接下来就在我们创建的子网落spark中规划集群中每个容器的ip地址。网络ip分配如下:</p><p>hadoop-master 172.16.0.2</p><p>hadoop-node1 172.16.0.3</p><p>hadoop-node2 172.16.0.4</p><h3 id="软件版本"><a href="#软件版本" class="headerlink" title="软件版本"></a>软件版本</h3><p>网络规划好了，首先Spark我们使用最新的2.4.4版本，Hadoop采用比较稳定的hadoop-2.7.3版本，scala采用scala-2.11.8，JDK采用jdk-8u101-linux-x64。</p><h3 id="SSH无密钥登录规则配置"><a href="#SSH无密钥登录规则配置" class="headerlink" title="SSH无密钥登录规则配置"></a>SSH无密钥登录规则配置</h3><p>注意这里不使用ssh-keygen -t rsa -P ‘’这种方式生成id_rsa.pub，然后集群节点互拷贝id_rsa.pub到authorized_keys文件这种方式，而 是通过在.ssh目录下配置ssh_conf文件的方式，ssh_conf中可以配置SSH的通信规则，例如以正则表达式的方式指定hostname为XXX的 机器之间实现互联互通，而不进行额外的密钥验证。为了编写这个正则表达式，我们5个节点的hostname都以hadoop-*的方式作为开 头，这就是采用这种命名规则的原因。下面来看下ssh_conf配置的内容:</p><pre><code class="shell">Host localhost    StrictHostKeyChecking noHost 0.0.0.0     StrictHostKeyChecking noHost hadoop-*     StrictHostKeyChecking no</code></pre><p>注意上面的最后一行，Host hadoop-* 指定了它的严格的Host验证StrictHostKeyChecking 为no，这样既可以是这5个hostname以 hadoop-*开头的容器之间实现互联互通，而不需要二外的验证。</p><h3 id="构建镜像"><a href="#构建镜像" class="headerlink" title="构建镜像"></a>构建镜像</h3><p>Dockerfile编写完成，接下来写一个build.sh脚本，内容如下:</p><pre><code class="shell"> echo build hadoop images docker build -t=&quot;spark&quot; . </code></pre><p>表示构建一个名叫spark的镜像，.表示Dockerfile的路径，因为在当前路径下，所有用.,若在其他地方则用绝对路径指定Dockerfile的路径 即可。</p><p>运行sh build.sh，就会开始制作镜像了。</p><h2 id="集群运行"><a href="#集群运行" class="headerlink" title="集群运行"></a>集群运行</h2><h3 id="启动容器-start-container-sh"><a href="#启动容器-start-container-sh" class="headerlink" title="启动容器 start_container.sh"></a>启动容器 start_container.sh</h3><p>使用这个镜像可完成容器的启动，因为使用了基于DockerNetworking的网络机制，因此可以在启动容器的时候为容器在子网172.16.0.0/16 spark中分贝172.16.0.1 172.16.0.255以外的IP地址，容器内部容器的通信是基于hostname，因此 需要指定hostname，为了方便容器的管理，需要为启动的每个容器指定一个名字。为了方便外网访问，需要通过-p命令指定容器到宿主机的端口映射。还要为每个容器增加host列表。</p><pre><code class="shell"># hadoop-masterdocker run -itd --restart=always \    --net spark \    --ip 172.16.0.2 \    --privileged \    -p 18032:8032 \    -p 28080:18080 \    -p 29888:19888 \    -p 17077:7077 \    -p 51070:50070 \    -p 18888:8888 \    -p 19000:9000 \    -p 11100:11000 \    -p 51030:50030 \    -p 18050:8050 \    -p 18081:8081 \    -p 18900:8900 \    --name hadoop-master \    --hostname hadoop-master \    --add-host hadoop-node1:172.16.0.3 \    --add-host hadoop-node2:172.16.0.4 \    --add-host hadoop-mysql:172.16.0.6 \    spark /usr/sbin/init# hadoop-node1docker run -itd --restart=always \    --net spark \    --ip 172.16.0.3 \    --privileged \    -p 18042:8042 \    -p 51010:50010 \    -p 51020:50020 \    --name hadoop-node1 \    --hostname hadoop-node1 \    --add-host hadoop-master:172.16.0.2 \    --add-host hadoop-node2:172.16.0.4 \    spark /usr/sbin/init# hadoop-node2docker run -itd --restart=always \    --net spark \    --ip 172.16.0.4 \    --privileged \    -p 18043:8042 \    -p 51011:50011 \    -p 51021:50021 \    --name hadoop-node2 \    --hostname hadoop-node2 \    --add-host hadoop-master:172.16.0.2 \    --add-host hadoop-node1:172.16.0.3 \    spark /usr/sbin/init</code></pre><h3 id="关闭集群-stop-container-sh"><a href="#关闭集群-stop-container-sh" class="headerlink" title="关闭集群 stop_container.sh"></a>关闭集群 stop_container.sh</h3><pre><code class="shell">echo stop containersdocker stop hadoop-masterdocker stop hadoop-node1docker stop hadoop-node2echo remove containersdocker rm hadoop-masterdocker rm hadoop-node1docker rm hadoop-node2echo rm containersdocker ps</code></pre><h3 id="重启集群-restart-container-sh"><a href="#重启集群-restart-container-sh" class="headerlink" title="重启集群 restart_container.sh"></a>重启集群 restart_container.sh</h3><pre><code class="shell">echo stop containersdocker stop hadoop-masterdocker stop hadoop-node1docker stop hadoop-node2echo restart containersdocker start hadoop-masterdocker start hadoop-node1docker start hadoop-node2echo start sshddocker exec -it hadoop-master systemctl start sshddocker exec -it hadoop-node1 systemctl start sshddocker exec -it hadoop-node2 systemctl start sshddocker exec -it hadoop-master ~/restart-hadoop.shecho  containers starteddocker ps</code></pre><h2 id="Trouble-Shooting"><a href="#Trouble-Shooting" class="headerlink" title="Trouble Shooting"></a>Trouble Shooting</h2><ul><li><p>docker里面执行systemctl报错</p><p>解决方案：启动的时候用/usr/sbin/init</p></li><li><p>docker登录harbor报错<br>window直接修改docker设置，添加52.83.79.244:8093到docker insecure registry<br>linux修改方法：<a href="https://blog.csdn.net/u010397369/article/details/42422243" target="_blank" rel="noopener">https://blog.csdn.net/u010397369/article/details/42422243</a>  </p></li></ul><h2 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h2><p>进入hadoop-master容器内部，执行spark-shell</p><pre><code class="shell">root@node-2 docker-spark]# docker exec -it hadoop-master /bin/bash[root@hadoop-master ~]# sparkspark-class   spark-shell   spark-sql     spark-submit  sparkR[root@hadoop-master ~]# spark-shell19/12/03 04:51:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableSetting default log level to &quot;WARN&quot;.To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).19/12/03 04:51:43 WARN util.Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.Spark context Web UI available at http://hadoop-master:4040Spark context available as &#39;sc&#39; (master = spark://hadoop-master:7077, app id = app-20191203045141-0000).Spark session available as &#39;spark&#39;.Welcome to      ____              __     / __/__  ___ _____/ /__    _\ \/ _ \/ _ `/ __/  &#39;_/   /___/ .__/\_,_/_/ /_/\_\   version 2.4.4      /_/Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_101)Type in expressions to have them evaluated.Type :help for more information.scala&gt;</code></pre><h2 id="本地部署"><a href="#本地部署" class="headerlink" title="本地部署"></a>本地部署</h2><ul><li><p>想要在自己的笔记本环境使用</p><p>首先笔记本环境下需要有docker环境</p><ul><li>拉取gitlab上面的项目：git clone <a href="mailto:git@161.189.27.8">git@161.189.27.8</a>:chenliang/docker-spark.git</li><li>拉去harbor上面的镜像：docker pull 52.83.79.244:8093/wuhan/spark:v1（也可以自己构建镜像，Dockerfile文件在gitlab项目里边）<br>  前提：机器docker环境登录harbor，账号密码：admin 1qaz!QAZ<br>docker login 52.83.79.244:8093<br>登录报错：Error response from daemon: Get <a href="https://52.83.79.244:8093/v2/" target="_blank" rel="noopener">https://52.83.79.244:8093/v2/</a>: http: server gave HTTP response to HTTPS client<br>解决：参见trouble shooting</li><li>使用相关脚本执行启动、停止、重启集群</li></ul></li></ul><ul><li><p>如何自己的spark程序如何在docker环境下执行  </p><p>启动spark的集群之后，使用docker cp等命令将打好的jar包打进容器内，使用spark脚本执行</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>运维</category>
      
    </categories>
    
    
    <tags>
      
      <tag>docker</tag>
      
      <tag>环境</tag>
      
      <tag>大数据</tag>
      
      <tag>部署</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker部署Nextcloud</title>
    <link href="/2020/01/09/Docker%E9%83%A8%E7%BD%B2Nextcloud/"/>
    <url>/2020/01/09/Docker%E9%83%A8%E7%BD%B2Nextcloud/</url>
    
    <content type="html"><![CDATA[<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><h3 id="docker的安装及配置"><a href="#docker的安装及配置" class="headerlink" title="docker的安装及配置"></a>docker的安装及配置</h3><pre><code class="shell"># 通过yum安装yum install -y docker-ce# 启动docker并设置开机启动systemctl start dockersystemctl enable docker# aws ec2 linux2另外一种安装方式sudo amazon-linux-extras install dockersudo service docker start</code></pre><p>由于docker镜像源默认是在国外，拉取镜像速度非常慢，修改daemon配置文件/etc/docker/daemon.json来使用加速器</p><pre><code class="shell">mkdir -p /etc/dockertouch /etc/docker/daemon.jsonvim /etc/docker/daemon.json{  &quot;registry-mirrors&quot;: [&quot;https://b3sst9pc.mirror.aliyuncs.com&quot;]}systemctl daemon-reloadsystemctl restart docker</code></pre><h3 id="docker-compose安装"><a href="#docker-compose安装" class="headerlink" title="docker-compose安装"></a>docker-compose安装</h3><p>Docker Compose是 docker 提供的一个命令行工具，用来定义和运行由多个容器组成的应用。使用 compose，我们可以通过 YAML 文件声明式的定义应用程序的各个服务，并由单个命令完成应用的创建和启动。</p><ul><li><p>docker-compose命令安装</p><pre><code class="shell"># 安装pipyum -y install epel-releaseyum -y install python-pip# 确认版本pip --version# 更新pippip install --upgrade pip# 安装docker-composepip install docker-compose # 查看版本docker-compose version</code></pre></li></ul><h2 id="nextcloud的部署"><a href="#nextcloud的部署" class="headerlink" title="nextcloud的部署"></a>nextcloud的部署</h2><p>nextcloud通过docker-compose命令进行构建，创建 docker-compose.yml文件</p><pre><code class="shell"># 创建docker-compose.yml文件touch docker-compose.yml# 粘贴以下内容version: &#39;2&#39;services:  db:    image: mariadb    restart: always    volumes:      - /data/mariadb:/var/lib/mysql    environment:      - MYSQL_ROOT_PASSWORD=root      - MYSQL_PASSWORD=nextcloud      - MYSQL_DATABASE=nextcloud      - MYSQL_USER=nextcloud  app:    image: nextcloud    restart: always    ports:      - 8091:80    links:      - db    volumes:      - /data/nextcloud/data:/var/www/html/data          - /data/nextcloud/themes:/var/www/html/themes      - /data/nextcloud/apps:/var/www/html/custom_apps</code></pre><p>docker-compose.yml文件说明</p><p>通过启动两个service服务mariadb及nextcloud服务，并通过link连接到一起，其中将/var/www/html/data、/var/www/html/themes、/var/www/html/custom_apps映射到linux服务器指定目录实现数据持久化，这样下次docker重启的时候数据不会发生丢失。</p><p>nextcloud实现预览编辑office文件需要插件onlyoffice支持，通过docker创建onlyoffice服务器，并启动nextcloud配置onlyoffice</p><pre><code class="shell">docker run -it -d -p 8061:80 onlyoffice/documentserver  -v /data/onlyoffice/logs:/var/log/onlyoffice /data/onlyoffice/data:/var/www/onlyoffice/Data /data/onlyoffice/lib:/var/lib/onlyoffice /data/onlyoffice/db:/var/lib/postgresql</code></pre><p>启动nextcloud、停止nextcloud、查看nextcloud启动状态</p><pre><code class="shell"># 启动nextclouddocker-compose up -d# 查看nextcloud状态docker-compose ps# 停止nextclouddocker-compose stop</code></pre><h2 id="nextcloud的数据迁移"><a href="#nextcloud的数据迁移" class="headerlink" title="nextcloud的数据迁移"></a>nextcloud的数据迁移</h2><p>当需要nextcloud迁移到另外一台服务的时候，需要将nextcloud持久化的数据通过scp或者其他方式复制到另外一台机器，将原来的机器mariadb的nextcloud数据库进行导出，在新的机器上面导入数据库数据。复制YAML文件到新的机器，通过docker-compose进行启动，进入容器内部修改持久化数据的权限及修改nextcloud配置文件，最后重启容器。</p><pre><code class="shell"># 复制持久化数据到新的机器上面scp -R /data/nextcloud root@192.168.12.1:/data# 导出原来机器的上面的mariadb数据docker exec -it  nextcloud_db_1【docker容器名称/ID】 mysqldump -uroot -proot【数据库密码】 nextcloud【数据库名称】 &gt; /opt/sql_bak/nextcloud.sql【导出表格路径】# 将sql文件复制到新的机器上面scp  /opt/sql_bak/nextcloud.sql root@192.168.12.1:/opt/sql_bak# 在新的机器上面执行该sql文件(需要先启动docker容器)docker cp /opt/sql_bak/nextcloud.sql 【容器名】:/root/docker exec -it 【容器名/ID】shmysql -uroot -p nextcloud【数据库名】 &lt; /root/nextcloud.sql# 通过docker-compose启动镜像，修改权限 docker exec -it -u root nextcloud_app_1【容器id/容器名】 /bin/bash root@bafc02ce112a:/var/www/html# chown -R www-data:root /var/www/html/ root@bafc02ce112a:exit# 修改nextcloud配置文件vim /data/nextcloud/config/config.php# 修改ip&#39;trusted_domains&#39; =&gt;  array (    0 =&gt; &#39;161.189.27.8:8091&#39;,  ),  &#39;datadirectory&#39; =&gt; &#39;/var/www/html/data&#39;,  &#39;dbtype&#39; =&gt; &#39;mysql&#39;,  &#39;version&#39; =&gt; &#39;16.0.4.1&#39;,  &#39;overwrite.cli.url&#39; =&gt; &#39;http://161.189.27.8:8091&#39;,# 重启docker-compose restart</code></pre><h2 id="trouble-shooting"><a href="#trouble-shooting" class="headerlink" title="trouble shooting"></a>trouble shooting</h2><ul><li><p>安装docker-compose命令执行pip install docker-compose的时候报以下错误</p><pre><code>ERROR: Cannot uninstall &#39;requests&#39;. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.</code></pre><p>解决版本，强制安装requests包</p><pre><code class="shell"> pip install --ignore-installed requests</code></pre><p>再重新执行</p><pre><code class="shell">pip install docker-compose</code></pre></li><li><p>docker-compose报错：No module named ssl_match_hostname</p><pre><code>File &quot;/usr/local/lib/python2.7/dist-packages/docker/transport/ssladapter.py&quot;, line 23, in &lt;module&gt;from backports.ssl_match_hostname import match_hostnameImportError: No module named ssl_match_hostname</code></pre><p>原因：</p><p><strong>/usr/local/lib/python2.7/distpackages/docker/transport/ssladapter.py **<br>在包路径下找不到 **backports包里的ssl_match_hostname</strong>模块</p><p>解决办法</p><pre><code class="shell">#进入backports模块路径cd /usr/lib/python2.7/site-packages#复制整个包到transport包路径下cp -r backports /usr/lib/python2.7/site-packages/docker/transport</code></pre></li></ul>]]></content>
    
    
    <categories>
      
      <category>运维</category>
      
    </categories>
    
    
    <tags>
      
      <tag>docker</tag>
      
      <tag>nextcloud</tag>
      
      <tag>网盘</tag>
      
      <tag>部署</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker私服Harbor使用指南</title>
    <link href="/2020/01/09/Harbor%E9%83%A8%E7%BD%B2/"/>
    <url>/2020/01/09/Harbor%E9%83%A8%E7%BD%B2/</url>
    
    <content type="html"><![CDATA[<h2 id="harbor搭建"><a href="#harbor搭建" class="headerlink" title="harbor搭建"></a>harbor搭建</h2><p>官方提供两种方式安装harbor，离线及在线方式。本文档描述离线方式进行安装</p><p>查看docker-compose版本，若版本低进行升级安装，</p><p>下载官方安装包</p><pre><code class="shell">cd /optwget https://storage.googleapis.com/harbor-releases/release-1.9.0/harbor-offline-installer-v1.9.2-rc1.tgz# 解压tar zxvf harbor-offline-installer-v1.9.2-rc1.tgz# 查看cd harborls[root@ip-172-31-23-16 harbor]# lltotal 623296drwxr-xr-x 3 root root        20 Nov  5 05:50 common-rw-r--r-- 1 root root      5369 Nov  5 06:07 docker-compose.yml-rw-r--r-- 1 root root 638214056 Nov  1 03:14 harbor.v1.9.2.tar.gz-rw-r--r-- 1 root root      5816 Nov  5 06:07 harbor.yml-rwxr-xr-x 1 root root      5088 Nov  1 03:13 install.sh-rw-r--r-- 1 root root     11347 Nov  1 03:13 LICENSE-rwxr-xr-x 1 root root      1748 Nov  1 03:13 prepare</code></pre><p>修改配置，主要修改两个文件</p><p>harbor.yml为系统配置文件，docker-compose.yml为docker相关配置文件</p><p>修改harbor.yml</p><pre><code class="shell">vim harbor.yml# 修改hostnamehostname: 52.83.79.244# 修改映射端口http:  port: 8093# 修改admin password（可选）harbor_admin_password: 1qaz!QAZ# 修改数据库相关参数database:  password: root  max_idle_conns: 50  max_open_conns: 100# 修改数据持久化host目录data_volume: /data/harbor# 修改log目录 location: /var/log/harbor</code></pre><p>修改docker-compose.yml文件</p><pre><code class="yaml">vim docker-compose.yml# 添加ports映射registry:    image: goharbor/registry-photon:v2.7.1-patch-2819-2553-v1.9.2    container_name: registry    restart: always    ports:      - 5000:5000</code></pre><p>启动harbor</p><pre><code>./install.sh</code></pre><p>安装之后查看</p><pre><code class="shell">docker-compose ps     Name                     Command                  State                 Ports---------------------------------------------------------------------------------------------harbor-core         /harbor/harbor_core              Up (healthy)harbor-db           /docker-entrypoint.sh            Up (healthy)   5432/tcpharbor-jobservice   /harbor/harbor_jobservice  ...   Up (healthy)harbor-log          /bin/sh -c /usr/local/bin/ ...   Up (healthy)   127.0.0.1:1514-&gt;10514/tcpharbor-portal       nginx -g daemon off;             Up (healthy)   8080/tcpnginx               nginx -g daemon off;             Up (healthy)   0.0.0.0:8093-&gt;8080/tcpredis               redis-server /etc/redis.conf     Up (healthy)   6379/tcpregistry            /entrypoint.sh /etc/regist ...   Restartingregistryctl         /harbor/start.sh                 Up (healthy)</code></pre><p>登录harbor ，账号密码为harbor.yml所设置的密码</p><p><a href="http://52.83.79.244:8093/harbor/projects" target="_blank" rel="noopener">http://52.83.79.244:8093/harbor/projects</a></p><p>登录时候还不能直接利用docker push 到服务器，这是因为 docker1.3.2 版本开始默认 docker registry 使用的是 https，我们设置 Harbor 默认 http 方式，所以当执行用 docker login、pull、push 等命令操作非 https 的 docker regsitry 的时就会报错。解决办法：</p><pre><code class="shell">vim /usr/lib/systemd/system/docker.service# ExecStart 增加 --insecure-registry=52.83.79.244【配置文件中的hostname】ExecStart=/usr/bin/dockerd $OPTIONS $DOCKER_STORAGE_OPTIONS $DOCKER_ADD_RUNTIMES --insecure-registry=52.83.79.244:8093# 重启服务systemctl daemon-reloadsystemctl restart docker</code></pre><p>harbor常用命令</p><pre><code class="shell"># 需要进入harbor文件夹执行cd /opt/harbordocker-compose up -d               ###后台启动，如果容器不存在根据镜像自动创建docker-compose down   -v           ###停止容器并删除容器docker-compose start               ###启动容器，容器不存在就无法启动，不会自动创建镜像docker-compose stop                ###停止容器</code></pre><h2 id="使用及配置"><a href="#使用及配置" class="headerlink" title="使用及配置"></a>使用及配置</h2><ul><li><p>docker 登录到harbor</p><pre><code class="shell">[root@ip-172-31-23-16 harbor]# docker login 52.83.79.244:8093Username: adminPassword:WARNING! Your password will be stored unencrypted in /root/.docker/config.json.Configure a credential helper to remove this warning. Seehttps://docs.docker.com/engine/reference/commandline/login/#credentials-storeLogin Succeeded</code></pre></li></ul><p>登录harbor后创建一个公共仓库，命名为wuhan</p><p>将镜像发布到harbor</p><pre><code class="shell"># 查看需要上传的镜像docker images# 为镜像上tagdocker tag jenkins/jenkins:lts 52.83.79.244:8093/wuhan/jenkins:lts# 上传到wuhan库docker push jenkins/jenkins:lts 52.83.79.244:8093/wuhan/jenkins:lts# 若需要上传到library库docker push jenkins/jenkins:lts 52.83.79.244:8093/library/jenkins:lts</code></pre><h2 id="配置https协议"><a href="#配置https协议" class="headerlink" title="配置https协议"></a>配置https协议</h2><pre><code class="shell"># 准备工作mkdir -p /data/harbor-certyum install -y openssl------------cd data/harbor-certopenssl genrsa -des3 -out server.key 2048 openssl req -new -key server.key -out server.csr cp server.key server.key.org openssl rsa -in server.key.org -out server.key openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt </code></pre><p>生成证书之后，修改harbor.yaml文件<br>具体配置如下<br><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/WX20191219-155232@2x.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/WX20191219-155232@2x.png" srcset="/img/loading.gif" alt="WX20191219-155232@2x.png"></a></p><h2 id="使用及配置-1"><a href="#使用及配置-1" class="headerlink" title="使用及配置"></a>使用及配置</h2><ul><li><p>docker 登录到harbor</p><pre><code class="shell">[root@ip-172-31-23-16 harbor]# docker login 52.83.79.244:8093Username: adminPassword:WARNING! Your password will be stored unencrypted in /root/.docker/config.json.Configure a credential helper to remove this warning. Seehttps://docs.docker.com/engine/reference/commandline/login/#credentials-storeLogin Succeeded</code></pre></li></ul><p>登录harbor后创建一个公共仓库，命名为wuhan</p><p>将镜像发布到harbor</p><pre><code class="shell"># 查看需要上传的镜像docker images# 为镜像上tagdocker tag jenkins/jenkins:lts 52.83.79.244:8093/wuhan/jenkins:lts# 上传到wuhan库docker push jenkins/jenkins:lts 52.83.79.244:8093/wuhan/jenkins:lts# 若需要上传到library库docker push 52.83.79.244:8093/library/jenkins:lts</code></pre><h2 id="trouble-shooting"><a href="#trouble-shooting" class="headerlink" title="trouble shooting"></a>trouble shooting</h2><ul><li>docker登录harbor报错<br>window直接修改docker设置，添加52.83.79.244:8093到docker insecure registry<br>linux修改方法：<a href="https://blog.csdn.net/u010397369/article/details/42422243" target="_blank" rel="noopener">https://blog.csdn.net/u010397369/article/details/42422243</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>运维</category>
      
    </categories>
    
    
    <tags>
      
      <tag>https</tag>
      
      <tag>证书</tag>
      
      <tag>harbor</tag>
      
      <tag>docker</tag>
      
      <tag>部署</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>K8s监控部署方案</title>
    <link href="/2020/01/07/K8s%E7%9B%91%E6%8E%A7%E9%83%A8%E7%BD%B2%E6%96%B9%E6%A1%88/"/>
    <url>/2020/01/07/K8s%E7%9B%91%E6%8E%A7%E9%83%A8%E7%BD%B2%E6%96%B9%E6%A1%88/</url>
    
    <content type="html"><![CDATA[<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p><img src="https://i.loli.net/2020/01/08/DVBiRAwaNsnYbgL.png" srcset="/img/loading.gif" alt="grafana"></p><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><p><strong>yaml文件地址 👉🏻   <a href="https://161.189.27.8:8090/dqdev/pythogoras/tree/master/k8s-yaml/prometheus" target="_blank" rel="noopener">pythagoras</a></strong></p><h3 id="1-在k8s集群中创建namespace"><a href="#1-在k8s集群中创建namespace" class="headerlink" title="1 在k8s集群中创建namespace"></a>1 在k8s集群中创建namespace</h3><pre><code class="yaml">apiVersion: v1kind: Namespacemetadata:   name: ns-monitor  labels:    name: ns-monitorkubectl apply -f namespace.yaml</code></pre><h3 id="2-安装node-exporter"><a href="#2-安装node-exporter" class="headerlink" title="2 安装node-exporter"></a>2 安装node-exporter</h3><p>在kubernetest集群中部署node-exporter，Node-exporter用于采集kubernetes集群中各个节点的物理指标，比如：Memory、CPU等。可以直接在每个物理节点是直接安装，这里我们使用DaemonSet部署到每个节点上，使用 hostNetwork: true 和 hostPID: true 使其获得Node的物理指标信息，配置tolerations使其在master节点也启动一个pod。</p><p>node-exporter.yaml</p><pre><code class="yaml">kind: DaemonSetapiVersion: apps/v1beta2metadata:   labels:    app: node-exporter  name: node-exporter  namespace: ns-monitorspec:  revisionHistoryLimit: 10  selector:    matchLabels:      app: node-exporter  template:    metadata:      labels:        app: node-exporter    spec:      containers:        - name: node-exporter          image: prom/node-exporter:v0.16.0          ports:            - containerPort: 9100              protocol: TCP              name:    http      hostNetwork: true      hostPID: true      tolerations:        - effect: NoSchedule          operator: Exists---kind: ServiceapiVersion: v1metadata:  labels:    app: node-exporter  name: node-exporter-service  namespace: ns-monitorspec:  ports:    - name:    http      port: 9100      nodePort: 31672      protocol: TCP  type: NodePort  selector:    app: node-exporter</code></pre><pre><code class="shell">kubectl apply -f node-exporter.yaml</code></pre><p><strong>*检查是否执行成功(对应pod及svc)</strong> 👉🏻 **</p><pre><code class="shell">➜  ~ kubectl get pod -n ns-monitorNAME                          READY   STATUS    RESTARTS   AGEgrafana-547699f75-lxljq       1/1     Running   0          3h37mnode-exporter-75nmc           0/1     Pending   0          20hnode-exporter-t29kx           1/1     Running   0          20hnode-exporter-z6s7x           1/1     Running   0          20hprometheus-7d7654554d-f5fvf   1/1     Running   0          3h45m➜  ~ kubectl get svc -n ns-monitorNAME                    TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGEgrafana-service         NodePort   10.1.242.56    &lt;none&gt;        3000:31026/TCP   3h38mnode-exporter-service   NodePort   10.1.130.7     &lt;none&gt;        9100:31672/TCP   20hprometheus-service      NodePort   10.1.133.130   &lt;none&gt;        9090:30753/TCP   3h45m</code></pre><p><img src="https://i.loli.net/2020/01/08/TP6tIlvFKhgzHuZ.png" srcset="/img/loading.gif" alt="image-20200103143320329"></p><h3 id="3-部署Prometheus-pod"><a href="#3-部署Prometheus-pod" class="headerlink" title="3 部署Prometheus pod"></a>3 部署Prometheus pod</h3><p>prometheus.yaml 中包含rbac认证、ConfigMap等</p><pre><code class="shell">kubectl apply -f prometheus.yaml </code></pre><p><em>检查是否执行成功(对应pod及svc)*</em> 👉🏻 </p><pre><code class="shell">➜  ~ kubectl get pod -n ns-monitorNAME                          READY   STATUS    RESTARTS   AGEgrafana-547699f75-lxljq       1/1     Running   0          3h37mnode-exporter-75nmc           0/1     Pending   0          20hnode-exporter-t29kx           1/1     Running   0          20hnode-exporter-z6s7x           1/1     Running   0          20hprometheus-7d7654554d-f5fvf   1/1     Running   0          3h45m➜  ~ kubectl get svc -n ns-monitorNAME                    TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGEgrafana-service         NodePort   10.1.242.56    &lt;none&gt;        3000:31026/TCP   3h38mnode-exporter-service   NodePort   10.1.130.7     &lt;none&gt;        9100:31672/TCP   20hprometheus-service      NodePort   10.1.133.130   &lt;none&gt;        9090:30753/TCP   3h45m</code></pre><p><img src="https://i.loli.net/2020/01/08/fdFs8EN2xhHwCM6.png" srcset="/img/loading.gif" alt="image-20200103143645494"></p><h3 id="4-在k8s中部署grafana"><a href="#4-在k8s中部署grafana" class="headerlink" title="4 在k8s中部署grafana"></a>4 在k8s中部署grafana</h3><pre><code class="shell">kubectl apply -f grafana.yaml</code></pre><p><strong>检查是否执行成功(对应pod及svc)</strong> 👉🏻 </p><pre><code class="shell">➜  ~ kubectl get pod -n ns-monitorNAME                          READY   STATUS    RESTARTS   AGEgrafana-547699f75-lxljq       1/1     Running   0          3h37mnode-exporter-75nmc           0/1     Pending   0          20hnode-exporter-t29kx           1/1     Running   0          20hnode-exporter-z6s7x           1/1     Running   0          20hprometheus-7d7654554d-f5fvf   1/1     Running   0          3h45m➜  ~ kubectl get svc -n ns-monitorNAME                    TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGEgrafana-service         NodePort   10.1.242.56    &lt;none&gt;        3000:31026/TCP   3h38mnode-exporter-service   NodePort   10.1.130.7     &lt;none&gt;        9100:31672/TCP   20hprometheus-service      NodePort   10.1.133.130   &lt;none&gt;        9090:30753/TCP   3h45m</code></pre><h3 id="5-配置grafana数据源"><a href="#5-配置grafana数据源" class="headerlink" title="5 配置grafana数据源"></a>5 配置grafana数据源</h3><p>把prometheus配置成数据源 ：<a href="http://prometheus-service.ns-monitor:9090" target="_blank" rel="noopener">http://prometheus-service.ns-monitor:9090</a></p><p><img src="https://i.loli.net/2020/01/08/fzxnrR5iguDjlHk.png" srcset="/img/loading.gif" alt="image-20200103144416930"></p><h3 id="6-倒入dashboard"><a href="#6-倒入dashboard" class="headerlink" title="6 倒入dashboard"></a>6 倒入dashboard</h3><p>把 kubernetes的Dashboard的模板导入进来，直接把JSON格式内容复制进来。</p><p><img src="https://i.loli.net/2020/01/08/6XkA5hEjN1OWioS.png" srcset="/img/loading.gif" alt="image-20200103145516630"></p><h2 id="效果图"><a href="#效果图" class="headerlink" title="效果图"></a>效果图</h2><p><img src="https://i.loli.net/2020/01/08/bqEolIi8KVSGuHk.png" srcset="/img/loading.gif" alt="image-20200103145627198"></p><h2 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h2><ul><li><p><a href="https://jimmysong.io/kubernetes-handbook/practice/using-prometheus-to-monitor-kuberentes-cluster.html" target="_blank" rel="noopener">使用Prometheus监控kubernetes集群</a></p></li><li><p><a href="https://www.jianshu.com/p/ac8853927528" target="_blank" rel="noopener">k8s安装Prometheus+Grafana</a></p></li><li><p><a href="https://github.com/giantswarm/kubernetes-prometheus" target="_blank" rel="noopener">github-kubernetes-promethues</a></p></li><li><p><a href="https://github.com/giantswarm/prometheus" target="_blank" rel="noopener">github-giantswarm-promethues</a></p></li></ul>]]></content>
    
    
    <categories>
      
      <category>kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>kubernetes</tag>
      
      <tag>grafana</tag>
      
      <tag>prometheus</tag>
      
      <tag>监控</tag>
      
      <tag>部署</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Gitlab开启Https</title>
    <link href="/2020/01/07/gitlab%E5%BC%80%E5%90%AFhttps/"/>
    <url>/2020/01/07/gitlab%E5%BC%80%E5%90%AFhttps/</url>
    
    <content type="html"><![CDATA[<h2 id="Gitlab开启Https"><a href="#Gitlab开启Https" class="headerlink" title="Gitlab开启Https"></a>Gitlab开启Https</h2><h3 id="建立认证目录"><a href="#建立认证目录" class="headerlink" title="建立认证目录"></a>建立认证目录</h3><pre><code class="shell">mkdir -p /etc/gitlab/sslchmod 700 /etc/gitlab/ssl</code></pre><h3 id="建立证书"><a href="#建立证书" class="headerlink" title="建立证书"></a>建立证书</h3><pre><code class="shell"># step 1 创建private key （记住输入的密码（Pass phrase））openssl genrsa -des3 -out /etc/gitlab/ssl/server.key 2048# step 2 生成 Certificate Requestopenssl req -new -key /etc/gitlab/ssl/gitlab.domain.com.key -out /etc/gitlab/ssl/server.csr</code></pre><blockquote><p> Enter Country Name CN<br> Enter State or Province Full Name HB<br> Enter City Name WuHan<br> Enter Organization Name<br> Enter Company Name EV-IV<br> Enter Organizational Unit Name<br> Enter server hostname i.e. URL gitlab.domain.com<br> Enter Admin Email Address<br> Skip Challenge Password (Hit Enter)<br> Skip Optional Company Name (Hit Enter)</p></blockquote><pre><code class="shell">#在加载SSL支持的Nginx并使用上述私钥时要除去刚才设置的口令： #step3 备份csr文件及去除命令，直接覆盖了server.key了openssl rsa -inserver.key.org -out server.key#step 4最后标记证书使用上述私钥和CSR：（把csr标记后转换成了crt nginx要用key和crt文件）openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt</code></pre><h3 id="修改gitlab配置"><a href="#修改gitlab配置" class="headerlink" title="修改gitlab配置"></a>修改gitlab配置</h3><pre><code class="shell">vim /etc/gitlab/gitlab.rbexternal_url &#39;https://161.189.27.8:8090/&#39;nginx[&#39;redirect_http_to_https&#39;]= truenginx[&#39;ssl_client_certificate&#39;] = &quot;/etc/gitlab/ssl/server.crt&quot;nginx[&#39;ssl_certificate&#39;]= &quot;/etc/gitlab/ssl/server.crt&quot;nginx[&#39;ssl_certificate_key&#39;]= &quot;/etc/gitlab/ssl/server.key&quot;</code></pre><h3 id="重启gitlab"><a href="#重启gitlab" class="headerlink" title="重启gitlab"></a>重启gitlab</h3><pre><code class="shell">gitlab-ctl reconfiguregitlab-ctl restart</code></pre><p>最后通过访问https地址进行访问测试</p><h3 id="相关问题"><a href="#相关问题" class="headerlink" title="相关问题"></a>相关问题</h3><p>Q1 由于ssl证书为自签证书 git clone报错</p><pre><code class="shell">git clone https://161.189.27.8:8090/dqdev/pythogoras.gitCloning into &#39;pythogoras&#39;...fatal: unable to access &#39;https://161.189.27.8:8090/dqdev/pythogoras.git/&#39;: SSL certificate problem: self signed certificate</code></pre><p>关闭git ssl验证</p><pre><code class="bash">git config --global http.sslVerify false 关闭git config --global http.sslVerify true  开启</code></pre>]]></content>
    
    
    <categories>
      
      <category>运维</category>
      
    </categories>
    
    
    <tags>
      
      <tag>gitlab</tag>
      
      <tag>ssl</tag>
      
      <tag>https</tag>
      
      <tag>证书</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
